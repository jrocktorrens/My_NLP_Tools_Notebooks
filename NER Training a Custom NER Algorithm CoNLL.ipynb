{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhhgpxxSl8Ka"
   },
   "source": [
    "# NER Training a Custom NER Algorithm\n",
    "\n",
    "In this exercise, we will train our own RNN-based Named Entity Recognition algorithm, using the CoNLL-2003 tagged dataset.\n",
    "\n",
    "## Part 1: Loading CoNLL-2003 data\n",
    "\n",
    "The [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) shared task was a joint effort by academics to provide approaches to named entity recognition, using a tagged dataset of named entities in English and German. We will be using the tagged English data from CoNLL-2003, found in the accompanying file *conll2003.zip*.\n",
    "\n",
    "After uploading this file to the current directory, access the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FuXHLev_7DzL",
    "outputId": "5ec02f19-6308-4c6e-c79a-f2f3bf8be483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  conll2003.zip\n",
      "   creating: conll2003/\n",
      "  inflating: conll2003/train.txt     \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/conll2003/\n",
      "  inflating: __MACOSX/conll2003/._train.txt  \n",
      "  inflating: conll2003/valid.txt     \n",
      "  inflating: __MACOSX/conll2003/._valid.txt  \n",
      "  inflating: conll2003/test.txt      \n",
      "  inflating: __MACOSX/conll2003/._test.txt  \n"
     ]
    }
   ],
   "source": [
    "! unzip conll2003.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PiEEWY2Gd6B4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_conll(filename):\n",
    "  df = pd.read_csv(filename,\n",
    "                     sep=' ', header=None, keep_default_na=False,\n",
    "                     names = ['TOKEN', 'POS', 'CHUNK', 'NE'],\n",
    "                     quoting=3, skip_blank_lines=False)\n",
    "  df['SENTENCE'] = (df.TOKEN == '').cumsum()\n",
    "  return df[df.TOKEN != '']\n",
    "train_df = read_conll('conll2003/train.txt')\n",
    "valid_df = read_conll('conll2003/valid.txt')\n",
    "test_df = read_conll('conll2003/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUC6Fe3IfeyU"
   },
   "source": [
    "Note that the CoNLL-2003 data contains part-of-speech (POS) and chunk tags, but we will only be using the token text and named entity (NE) tags that are provided.\n",
    "\n",
    "**Questions:**\n",
    "  1. What percentages of the CoNLL-2003 data are training, validation, and testing data? (calculate directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YW7qEfZn78HP",
    "outputId": "da8c9ad7-52c7-4f19-cbe6-ee2f081b5005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train percentage: 67.6%\n",
      "Validation percentage: 17.0%\n",
      "Test percentage: 15.4%\n"
     ]
    }
   ],
   "source": [
    "df_size = train_df.shape[0] + valid_df.shape[0] + test_df.shape[0]\n",
    "print(f\"Train percentage: {round((train_df.shape[0]/df_size)*100,1)}%\")\n",
    "print(f\"Validation percentage: {round((valid_df.shape[0]/df_size)*100,1)}%\")\n",
    "print(f\"Test percentage: {round((test_df.shape[0]/df_size)*100,1)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9r6bdi178HQ"
   },
   "source": [
    "  2. What do the tags in column 'NE' mean? Explain in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "4uRvJpyV78HR",
    "outputId": "c40e9bd5-97d8-44c1-e7d1-bf9bf90e6101"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>POS</th>\n",
       "      <th>CHUNK</th>\n",
       "      <th>NE</th>\n",
       "      <th>SENTENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-DOCSTART-</td>\n",
       "      <td>-X-</td>\n",
       "      <td>-X-</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EU</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rejects</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>B-VP</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>German</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>call</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TOKEN  POS CHUNK      NE  SENTENCE\n",
       "0  -DOCSTART-  -X-   -X-       O         0\n",
       "2          EU  NNP  B-NP   B-ORG         1\n",
       "3     rejects  VBZ  B-VP       O         1\n",
       "4      German   JJ  B-NP  B-MISC         1\n",
       "5        call   NN  I-NP       O         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uuxDkClq78HS",
    "outputId": "43d5a3f0-e8ff-4651-e2f5-783776e244e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_df.NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1l6f6yid78HT"
   },
   "source": [
    "* B-LOC - begin of location entity\n",
    "* I-LOC - inside of location entity\n",
    "* B-MISC - begin of miscellaneous entity\n",
    "* I-MISC - inside of miscellaneous entity\n",
    "* B-ORG - begin of organization entity\n",
    "* I-ORG - inside of organization entity\n",
    "* B-PER - begin of a person entity\n",
    "* I_PER - inside of a person entity\n",
    "* O - Outside of an entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt8U2DEdgpV4",
    "tags": []
   },
   "source": [
    "## Part 2: Feature calculation\n",
    "\n",
    "In order to learn named entity recognition using RNNs, we must transform our input and output into numeric vectors by calculating relevant features. For our basic NER algorithm, we will simply use word indices as input and one-hot embeddings of NER tags as output.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "3. Save a list of the 5000 most common word tokens (values from column `TOKEN`) in our training data as a list `vocab`, and save a list of all unique entity tags (values from column `NE`) as a list `ne_tags`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_95who6M78HU",
    "outputId": "b30a9c36-f523-4b6b-8d06-596bfbe86018"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'I-MISC', 'I-LOC', 'O', 'I-ORG', 'B-LOC', 'B-PER', 'B-MISC', 'I-PER']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count_words = Counter(train_df.TOKEN)\n",
    "vocab = [item[0] for item in sorted(count_words.items(), key=lambda item: item[1], reverse=True)]\n",
    "vocab = vocab[:5000]\n",
    "ne_tags = list(set(train_df.NE))\n",
    "ne_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7h3e5rZ78HW"
   },
   "source": [
    "4. Create a function `token2index(token)` that takes in the value of a word token and returns a unique integer. It should return 1 for any token which is not found in `vocab` (i.e. which is out-of-vocabulary) and a number >= 2 for every token found in `vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kV3vWZse78HW"
   },
   "outputs": [],
   "source": [
    "def token2index(token):\n",
    "    if token in vocab:\n",
    "        return vocab.index(token)+2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNSJKeIR78HX"
   },
   "source": [
    "5. Create a function `ne_tag2index(ne_tag)` which returns a unique integer >= 1 for every entity tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "938mOCX378HX"
   },
   "outputs": [],
   "source": [
    "def ne_tag2index(ne_tag):\n",
    "    if ne_tag in ne_tags:\n",
    "        return ne_tags.index(ne_tag)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcNZsNYv78HY"
   },
   "source": [
    "6. Add new columns `token_index` and `ne_index` to the CoNLL data DataFrames containing the values of `token2index()` and `ne_tag2index()` for each token and entity tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6EtzQHKD78HY"
   },
   "outputs": [],
   "source": [
    "train_df['token_index'] = train_df.TOKEN.apply(lambda row: token2index(row))\n",
    "train_df['ne_index'] = train_df.NE.apply(lambda row: ne_tag2index(row))\n",
    "\n",
    "valid_df['token_index'] = valid_df.TOKEN.apply(lambda row: token2index(row))\n",
    "valid_df['ne_index'] = valid_df.NE.apply(lambda row: ne_tag2index(row))\n",
    "\n",
    "test_df['token_index'] = test_df.TOKEN.apply(lambda row: token2index(row))\n",
    "test_df['ne_index'] = test_df.NE.apply(lambda row: ne_tag2index(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDJMVEPN78HY"
   },
   "source": [
    "7. Generate training data feature matrix `X_train` of size (14987, 50) as follows:\n",
    "  * Use `train_df.groupby('SENTENCE').token_index.apply(list)` to get a list of lists of token indices, one list for each sentence.\n",
    "  * Use `pad_sequences()` from `tensorflow.keras.preprocessing.sequence` to pad every list of token indices with the value `0` at the beginning so they are all of length 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clrDLZd778HZ",
    "outputId": "3dce0a10-5267-4446-88ee-ef15dd5895e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14987, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = train_df.groupby('SENTENCE').token_index.apply(list)\n",
    "X_train = pad_sequences(X_train, maxlen=50)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJWh0k1P78HZ"
   },
   "source": [
    "8. Generate output data feature matrix `Y_train` of size (14987, 50, 10) by applying the same method to the entity token indices (column `ne_index`), and then one-hot encoding using `to_categorical()` from `tensorflow.keras.utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQFolxWe78Ha",
    "outputId": "842e180f-f12d-4ef3-fb42-35de57d13aca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14987, 50, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Y_train = train_df.groupby('SENTENCE').ne_index.apply(list)\n",
    "Y_train = pad_sequences(Y_train, maxlen=50)\n",
    "Y_train = to_categorical(Y_train, num_classes=10)\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MftvylcQ78Ha"
   },
   "source": [
    "9. Apply 7-8 on the validation and testing data as well to generate matrices `X_valid`, `Y_valid`, `X_test`, `Y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MNVO7yOz78Ha"
   },
   "outputs": [],
   "source": [
    "X_valid = valid_df.groupby('SENTENCE').token_index.apply(list)\n",
    "X_valid = pad_sequences(X_valid, maxlen=50)\n",
    "\n",
    "Y_valid = valid_df.groupby('SENTENCE').ne_index.apply(list)\n",
    "Y_valid = pad_sequences(Y_valid, maxlen=50)\n",
    "Y_valid = to_categorical(Y_valid, num_classes=10)\n",
    "\n",
    "X_test = test_df.groupby('SENTENCE').token_index.apply(list)\n",
    "X_test = pad_sequences(X_test, maxlen=50)\n",
    "\n",
    "Y_test = test_df.groupby('SENTENCE').ne_index.apply(list)\n",
    "Y_test = pad_sequences(Y_test, maxlen=50)\n",
    "Y_test = to_categorical(Y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6G_vjMYx78Hb",
    "outputId": "e0b62319-2362-4c43-fad4-fdc95e62d54d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3466, 50), (3466, 50, 10), (3684, 50), (3684, 50, 10))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape, Y_valid.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhYllqeCrrCE",
    "tags": []
   },
   "source": [
    "## Part 3: Building and training the model\n",
    "\n",
    "Now we are ready to build our network that will predict NER tags from the inputted words. The architecture will be roughly similar to our previous exercise on RNNs.\n",
    "\n",
    "The following imports will help you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DhR9m-Nit5_k"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4VXf9qRt9oy"
   },
   "source": [
    "**Questions:**\n",
    "\n",
    "10. Build a sequential model `model` with the following layers:\n",
    "  * Embedding – use embedding dimension 200, and make sure to set `input_length=50` and `mask_zero=True` (to ignore the padding indices).\n",
    "  * LSTM – use hidden state dimension 128, and return the hidden state at each time step (`return_sequences=True`).\n",
    "  * Fully-connected layer (`Dense()`) with softmax activation. Hint: The output dimension of `Dense()` is the number of possible output labels, including the padding label `0`.\n",
    "\n",
    "  Compile the model with loss function `categorical_crossentropy` and optimizer `adam`, and using accuracy as a metric. Print a summary of the model (`model.summary()`). What is the expected shape of input for the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RJxz4RZX78Hc"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(output_dim=200, input_dim=5002, input_length=50, mask_zero=True),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "69yihirb78Hc"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M0ncLadZ78Hd",
    "outputId": "8b889236-1698-4ad4-c46f-0172c068ca40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 200)           1000400   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 128)           168448    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50, 10)            1290      \n",
      "=================================================================\n",
      "Total params: 1,170,138\n",
      "Trainable params: 1,170,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOdASewN78Hd",
    "outputId": "44212824-3400-4a48-ce34-8b9c7c20867f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the expected input for the model: (None, 50)\n"
     ]
    }
   ],
   "source": [
    "print(f'the expected input for the model: {model.input_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PqN5Dsl78Hd"
   },
   "source": [
    "11. Train the model on `X_train` and `Y_train`, using `X_valid`, and `Y_valid` as validation data. Use whatever batch size and number of epochs work best for you. Train the model until validation loss or accuracy starts increasing. How many epochs did you use for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXkzcKUu78Hd",
    "outputId": "45fb2320-c3fb-4ebf-a47a-f2b8c4b5c919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "235/235 [==============================] - 53s 198ms/step - loss: 0.1868 - accuracy: 0.8408 - val_loss: 0.1170 - val_accuracy: 0.8723\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 46s 198ms/step - loss: 0.0827 - accuracy: 0.9106 - val_loss: 0.0777 - val_accuracy: 0.9287\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 46s 196ms/step - loss: 0.0525 - accuracy: 0.9446 - val_loss: 0.0644 - val_accuracy: 0.9366\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 46s 194ms/step - loss: 0.0417 - accuracy: 0.9523 - val_loss: 0.0599 - val_accuracy: 0.9385\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 46s 195ms/step - loss: 0.0369 - accuracy: 0.9565 - val_loss: 0.0592 - val_accuracy: 0.9405\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 47s 198ms/step - loss: 0.0334 - accuracy: 0.9596 - val_loss: 0.0585 - val_accuracy: 0.9410\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 48s 203ms/step - loss: 0.0308 - accuracy: 0.9622 - val_loss: 0.0593 - val_accuracy: 0.9395\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49dcfd5e90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss',  patience=1, \n",
    "                         verbose=1, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid),\n",
    "          epochs=100, batch_size=64, callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7_XetMM78He"
   },
   "source": [
    "12. Create a model *model2* that is the same as *model* but with the LSTM layer wrapped by `Bidirectional()`, so the model becomes a BiLSTM model. How does this change the final validation loss? Does the model improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "GgX-Zoht78He"
   },
   "outputs": [],
   "source": [
    "model2 = Sequential([\n",
    "    Embedding(output_dim=200, input_dim=5002, input_length=50, mask_zero=True),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6dv8XHYm78He"
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKjkspYc78He",
    "outputId": "1abf98ed-8495-4b5d-ff74-d4bf38478bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "235/235 [==============================] - 95s 366ms/step - loss: 0.1737 - accuracy: 0.8487 - val_loss: 0.1040 - val_accuracy: 0.8980\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 82s 350ms/step - loss: 0.0599 - accuracy: 0.9366 - val_loss: 0.0596 - val_accuracy: 0.9440\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 82s 348ms/step - loss: 0.0355 - accuracy: 0.9627 - val_loss: 0.0516 - val_accuracy: 0.9509\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 82s 351ms/step - loss: 0.0268 - accuracy: 0.9714 - val_loss: 0.0459 - val_accuracy: 0.9564\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 83s 352ms/step - loss: 0.0220 - accuracy: 0.9759 - val_loss: 0.0473 - val_accuracy: 0.9566\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4965422f10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, Y_train, validation_data=(X_valid, Y_valid),\n",
    "          epochs=100,\n",
    "           batch_size=64,\n",
    "          callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVBsOAr778Hf"
   },
   "source": [
    "13. Compare the performance of the two models on the test set data `X_test` and `Y_test` (Hint: use `model.evaluate()`). Mention important metrics, overfitting, early stopping, etc... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FfWjrtIW78Hf",
    "outputId": "c6f942d2-febd-4fbb-d7c4-ef5f24619d6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 2s 18ms/step - loss: 0.0660 - accuracy: 0.9200\n",
      "116/116 [==============================] - 4s 37ms/step - loss: 0.0551 - accuracy: 0.9374\n",
      "model 1 evaluation: [0.06601439416408539, 0.9199905395507812]\n",
      "model 2 evaluation: [0.05508552864193916, 0.9373894929885864]\n"
     ]
    }
   ],
   "source": [
    "eval1 = model.evaluate(X_test, Y_test)\n",
    "eval2 = model2.evaluate(X_test, Y_test)\n",
    "\n",
    "print(f'model 1 evaluation: {eval1}\\nmodel 2 evaluation: {eval2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ow2yb6NRAgL3"
   },
   "source": [
    "comparing the performance, the model loss and accuracy values are better for the second model.  \n",
    "the first model has better metrics for validation vs test - it can indicates the model is overfitting.  \n",
    "it didnt happend in the second model, the second model, using biLSTM layer, looks better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0q7o8wJzUOb"
   },
   "source": [
    "## Running on custom input\n",
    "\n",
    "14. What does your model predict as NER tags for the following test sentences?\n",
    "\n",
    "Hint: Try using the following pipeline on each sentence:\n",
    "\n",
    "* Tokenize with nltk.word_tokenize()\n",
    "* Convert to array of indices with word2index() defined above\n",
    "* Pad to length 50 with pad_sequences() from Keras\n",
    "* Predict probabilities of NER tags with model2.predict()\n",
    "* Find maximum likelihood tags using np.argmax(), and ignore padding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5dXPLnjyzV7z"
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "  \"This is a test.\",\n",
    "  \"I live in the United States.\",\n",
    "  \"Israel is a country in the Middle East.\",\n",
    "  \"UK joins US in Gulf mission after Iran taunts American allies\",\n",
    "  \"The project was funded by the Portuguese Foundation for Science and Technology and the Israel Cancer Research Fund.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaWWJz-U78Hg",
    "outputId": "0c7b9a90-6155-456c-e3e2-04979992198d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "\n",
    "def predict(sentence):\n",
    "    token_indics = []\n",
    "    predictions = []\n",
    "    sentence_token = nltk.word_tokenize(sentence)\n",
    "    for token in sentence_token:\n",
    "        token_indics.append(token2index(token))\n",
    "    token_indics = pad_sequences([token_indics], maxlen=50)\n",
    "    pred = model2.predict(token_indics)\n",
    "    for i in pred[0][-len(sentence_token):]:\n",
    "        predictions.append(np.argmax(i))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lqggMM_78Hg",
    "outputId": "1ad1f3bb-5c2e-4dfe-ce6b-7d1e95f2b9eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test.\n",
      "O O O O O \n",
      "I live in the United States.\n",
      "O O O O B-LOC I-LOC O \n",
      "Israel is a country in the Middle East.\n",
      "B-LOC O O O O O B-LOC I-LOC O \n",
      "UK joins US in Gulf mission after Iran taunts American allies\n",
      "B-LOC B-LOC B-LOC O B-LOC O O B-LOC O I-MISC I-MISC \n",
      "The project was funded by the Portuguese Foundation for Science and Technology and the Israel Cancer Research Fund.\n",
      "O O O O O O B-MISC O O O O O O O B-LOC O I-ORG I-ORG O \n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "    pred = predict(sentence)\n",
    "    print(sentence)\n",
    "    for i in pred:\n",
    "        print(ne_tags[i-1], end=' ')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuKz5eC8zXvj"
   },
   "source": [
    "## Bonus: Adding features\n",
    "\n",
    "**Bonus question:**\n",
    "\n",
    "In (A) below, add code to add a new column 'SHAPE' to the dataset. This column should represent the shape of the word token by:\n",
    "* Replacing all capital letters with 'X'\n",
    "* Replacing all lowercase letters with 'x'\n",
    "* Replacing all digits with 'd'\n",
    "\n",
    "For example, we should have the following:\n",
    "\n",
    "* 'house' => 'xxxxx'\n",
    "* 'Apple' => 'Xxxxx'\n",
    "* 'R2D2' => 'XdXd'\n",
    "* 'U.K.' => 'X.X.'\n",
    "\n",
    "Hint: for a Pandas series. you can use series.str.replace() to easily replace text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "uLvg69DT0T6Q"
   },
   "outputs": [],
   "source": [
    "def series2shape(series):\n",
    "    series = series.str.replace('[a-z]', 'x', regex=True)\n",
    "    series = series.str.replace('[A-Z]', 'X', regex=True)\n",
    "    series = series.str.replace('\\d', 'd', regex=True)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "wTmXEg8__9Dy"
   },
   "outputs": [],
   "source": [
    "train_df['SHAPE'] = series2shape(train_df.TOKEN)\n",
    "valid_df['SHAPE'] = series2shape(valid_df.TOKEN)\n",
    "test_df['SHAPE'] = series2shape(test_df.TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "nnsWOr-m78Hh"
   },
   "outputs": [],
   "source": [
    "train_df['SHAPE'] = series2shape(train_df.TOKEN)\n",
    "valid_df['SHAPE'] = series2shape(valid_df.TOKEN)\n",
    "test_df['SHAPE'] = series2shape(test_df.TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OTRRNls0m0k"
   },
   "source": [
    "Once you complete this, run the following code to see how adding this as a feature improves the performance of the model. For simplicity we only use the top 100 word shapes. How does the final loss change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "OUeSBggL7J8P"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "shape_vocab = [w for w, f in Counter(train_df.SHAPE).most_common(100)]\n",
    "shape_set = set(shape_vocab)\n",
    "def shape2index(shape):\n",
    "  if shape in shape_set:\n",
    "    return shape_vocab.index(shape) + 2\n",
    "  else: # out-of-vocabulary shape\n",
    "    return 1\n",
    "\n",
    "n_words = 50\n",
    "def df2features2(df):\n",
    "  df['shape_index'] = df.SHAPE.apply(shape2index)\n",
    "  token_index_lists = df.groupby('SENTENCE').token_index.apply(list)\n",
    "  ne_index_lists = df.groupby('SENTENCE').ne_index.apply(list)\n",
    "  shape_index_lists = df.groupby('SENTENCE').ne_index.apply(list)\n",
    "  X = np.stack([\n",
    "      pad_sequences(token_index_lists, maxlen=n_words, value=0),\n",
    "      pad_sequences(shape_index_lists, maxlen=n_words, value=0)\n",
    "  ])\n",
    "  Y = to_categorical(pad_sequences(ne_index_lists, maxlen=n_words, value=0))\n",
    "  return X, Y\n",
    "\n",
    "X2_train, Y2_train = df2features2(train_df)\n",
    "X2_valid, Y2_valid = df2features2(valid_df)\n",
    "X2_test, Y2_test = df2features2(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "rAGrKU9K2lAB"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "\n",
    "input1 = Input(shape=(50,))\n",
    "input2 = Input(shape=(50,))\n",
    "embedded1 = Embedding(\n",
    "    len(vocab) + 2, 200,\n",
    "    input_length=50, mask_zero=True)(input1)\n",
    "embedded2 = Embedding(\n",
    "    len(shape_vocab) + 2, 8,\n",
    "    input_length=50, mask_zero=True)(input2)\n",
    "x = Concatenate()([embedded1, embedded2])\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "output = Dense(len(ne_tags) + 1, activation='softmax')(x)\n",
    "model3 = Model(inputs=[input1, input2], outputs=[output])\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uB9be01ssZL7",
    "outputId": "a661af85-aca2-400d-bf80-fdb6b3ade3a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "469/469 [==============================] - 176s 356ms/step - loss: 0.0940 - accuracy: 0.9156 - val_loss: 0.0093 - val_accuracy: 0.9946\n",
      "Epoch 2/100\n",
      "469/469 [==============================] - 167s 356ms/step - loss: 0.0027 - accuracy: 0.9985 - val_loss: 0.0012 - val_accuracy: 0.9992\n",
      "Epoch 3/100\n",
      "469/469 [==============================] - 161s 344ms/step - loss: 5.2870e-04 - accuracy: 0.9997 - val_loss: 6.1272e-04 - val_accuracy: 0.9996\n",
      "Epoch 4/100\n",
      "469/469 [==============================] - 165s 352ms/step - loss: 2.1788e-04 - accuracy: 0.9999 - val_loss: 3.8366e-04 - val_accuracy: 0.9997\n",
      "Epoch 5/100\n",
      "469/469 [==============================] - 164s 350ms/step - loss: 1.1210e-04 - accuracy: 1.0000 - val_loss: 3.6528e-04 - val_accuracy: 0.9996\n",
      "Epoch 6/100\n",
      "469/469 [==============================] - 170s 363ms/step - loss: 6.3669e-05 - accuracy: 1.0000 - val_loss: 2.1584e-04 - val_accuracy: 0.9998\n",
      "Epoch 7/100\n",
      "469/469 [==============================] - 173s 369ms/step - loss: 3.7542e-05 - accuracy: 1.0000 - val_loss: 1.7621e-04 - val_accuracy: 0.9998\n",
      "Epoch 8/100\n",
      "469/469 [==============================] - 163s 348ms/step - loss: 2.5075e-05 - accuracy: 1.0000 - val_loss: 1.8081e-04 - val_accuracy: 0.9998\n",
      "Epoch 9/100\n",
      "469/469 [==============================] - 161s 343ms/step - loss: 1.7072e-05 - accuracy: 1.0000 - val_loss: 1.2543e-04 - val_accuracy: 0.9999\n",
      "Epoch 10/100\n",
      "469/469 [==============================] - 162s 346ms/step - loss: 1.1875e-05 - accuracy: 1.0000 - val_loss: 1.0718e-04 - val_accuracy: 0.9999\n",
      "Epoch 11/100\n",
      "469/469 [==============================] - 170s 362ms/step - loss: 8.6210e-06 - accuracy: 1.0000 - val_loss: 9.9546e-05 - val_accuracy: 0.9999\n",
      "Epoch 12/100\n",
      "469/469 [==============================] - 161s 343ms/step - loss: 6.3504e-06 - accuracy: 1.0000 - val_loss: 7.9698e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "469/469 [==============================] - 164s 350ms/step - loss: 4.7232e-06 - accuracy: 1.0000 - val_loss: 6.9412e-05 - val_accuracy: 0.9999\n",
      "Epoch 14/100\n",
      "469/469 [==============================] - 168s 358ms/step - loss: 3.5335e-06 - accuracy: 1.0000 - val_loss: 6.0125e-05 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "469/469 [==============================] - 167s 355ms/step - loss: 2.6676e-06 - accuracy: 1.0000 - val_loss: 5.7137e-05 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "469/469 [==============================] - 167s 356ms/step - loss: 2.0288e-06 - accuracy: 1.0000 - val_loss: 5.4844e-05 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "469/469 [==============================] - 167s 356ms/step - loss: 1.5504e-06 - accuracy: 1.0000 - val_loss: 4.5448e-05 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "469/469 [==============================] - 168s 359ms/step - loss: 1.2007e-06 - accuracy: 1.0000 - val_loss: 4.8406e-05 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "469/469 [==============================] - 165s 352ms/step - loss: 9.2017e-07 - accuracy: 1.0000 - val_loss: 4.3612e-05 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "469/469 [==============================] - 173s 369ms/step - loss: 7.2133e-07 - accuracy: 1.0000 - val_loss: 3.4852e-05 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "469/469 [==============================] - 166s 354ms/step - loss: 5.6508e-07 - accuracy: 1.0000 - val_loss: 3.3226e-05 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "469/469 [==============================] - 169s 360ms/step - loss: 4.4112e-07 - accuracy: 1.0000 - val_loss: 3.3659e-05 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "469/469 [==============================] - 170s 362ms/step - loss: 3.4528e-07 - accuracy: 1.0000 - val_loss: 2.8330e-05 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "469/469 [==============================] - 168s 359ms/step - loss: 2.7564e-07 - accuracy: 1.0000 - val_loss: 2.8218e-05 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "469/469 [==============================] - 167s 355ms/step - loss: 2.1822e-07 - accuracy: 1.0000 - val_loss: 2.7655e-05 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "469/469 [==============================] - 172s 367ms/step - loss: 1.7313e-07 - accuracy: 1.0000 - val_loss: 2.5253e-05 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "469/469 [==============================] - 169s 359ms/step - loss: 1.3839e-07 - accuracy: 1.0000 - val_loss: 2.0018e-05 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "469/469 [==============================] - 170s 362ms/step - loss: 1.1089e-07 - accuracy: 1.0000 - val_loss: 2.2992e-05 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "469/469 [==============================] - 166s 355ms/step - loss: 8.9626e-08 - accuracy: 1.0000 - val_loss: 2.1617e-05 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "469/469 [==============================] - 165s 351ms/step - loss: 7.2153e-08 - accuracy: 1.0000 - val_loss: 1.7986e-05 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "469/469 [==============================] - 170s 363ms/step - loss: 5.8998e-08 - accuracy: 1.0000 - val_loss: 1.9271e-05 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "469/469 [==============================] - 170s 362ms/step - loss: 4.8342e-08 - accuracy: 1.0000 - val_loss: 1.8253e-05 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "469/469 [==============================] - 174s 372ms/step - loss: 3.9634e-08 - accuracy: 1.0000 - val_loss: 1.5228e-05 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "469/469 [==============================] - 168s 358ms/step - loss: 3.2801e-08 - accuracy: 1.0000 - val_loss: 1.5767e-05 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "469/469 [==============================] - 168s 359ms/step - loss: 2.7321e-08 - accuracy: 1.0000 - val_loss: 1.2435e-05 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "469/469 [==============================] - 173s 369ms/step - loss: 2.2803e-08 - accuracy: 1.0000 - val_loss: 1.3783e-05 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "469/469 [==============================] - 168s 358ms/step - loss: 1.9260e-08 - accuracy: 1.0000 - val_loss: 1.5150e-05 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "469/469 [==============================] - 170s 363ms/step - loss: 1.6355e-08 - accuracy: 1.0000 - val_loss: 1.3591e-05 - val_accuracy: 1.0000\n",
      "Epoch 00038: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f494f784bd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model3.fit(\n",
    "    [X2_train[0], X2_train[1]],\n",
    "    Y2_train,\n",
    "    validation_data=([X2_valid[0], X2_valid[1]], Y2_valid),\n",
    "    epochs=100, batch_size=32,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, verbose=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pU89CJ453NG_",
    "outputId": "80ea5d85-55ac-4474-b0ed-25d3e0240d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model3 loss on test data:\n",
      "116/116 [==============================] - 4s 38ms/step - loss: 4.6424e-04 - accuracy: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0004642434942070395, 0.9997628331184387]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model3 loss on test data:\")\n",
    "model3.evaluate([X2_test[0], X2_test[1]], Y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmMPaGsdpIth"
   },
   "source": [
    "model3 loss value is almost 0. the accuracy is extremly high with value of 0.999."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NER Exercise 2 - CoNLL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149e6d23-568f-4d0f-944b-0db0cdc3d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c33fb0-d6aa-42d2-85b1-d155757b5252",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433b9af1-c8c3-41a8-99ee-f17a3a95b909",
   "metadata": {},
   "source": [
    "Tokenization using nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a336e6-3268-46f6-8368-a8e9cece189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jrock/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# The text I used is a song named Mist Mountains\n",
    "lyrics = \"\"\"Far over the Misty Mountains cold To dungeons deep and caverns old\n",
    "We must away, ere break of day,\n",
    "To find our long forgotten gold.\n",
    "\n",
    "The pines were roaring on the height,\n",
    "The winds were moaning in the night.\n",
    "The fire was red, it flaming spread;\n",
    "The trees like torches blazed with light.\n",
    "\n",
    "The wind was on the withered heath,\n",
    "But in the forest stirred no leaf:\n",
    "There shadows lay be night or day,\n",
    "And dark things silent crept beneath.\n",
    " \n",
    "The wind went on from West to East;\n",
    "All movement in the forest ceased,\n",
    "But shrill and harsh across the marsh\n",
    "Its whistling voices were released.\n",
    "\n",
    "Farewell we call to hearth and hall!\n",
    "Though wind may blow and rain may fall,\n",
    "We must away ere break of day\n",
    "Far over the wood and mountain tall.\"\"\".replace('\\n', ' ')\n",
    "\n",
    "# Tokenizing sentences\n",
    "sentences = nltk.sent_tokenize(lyrics)\n",
    "# Tokenizing words\n",
    "words = nltk.word_tokenize(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ee6dd9-eda2-4143-b70f-3ca5dddaa640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the first 5 words: \n",
      "['Far', 'over', 'the', 'Misty', 'Mountains'].\n",
      "And here is the first sentence: \n",
      "['Far over the Misty Mountains cold To dungeons deep and caverns old We must away, ere break of day, To find our long forgotten gold.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Here are the first 5 words: \\n{words[:5]}.\\nAnd here is the first sentence: \\n{sentences[:1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9eb9c5-2f2a-4954-b59f-b0d55dc81646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 sentences, and 153 words.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(sentences)} sentences, and {len(words)} words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f9edd-a5cf-4514-897b-9f9096bf83db",
   "metadata": {},
   "source": [
    "How many unique tokens are there in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "256026e1-f6f3-48d2-94e8-306bff7ad954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are unique 99 words.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are unique {len(set(words))} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02079d47-db91-488e-867a-4f4dc1362282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we just split the text with pure python we have 135 words.\n"
     ]
    }
   ],
   "source": [
    "print(f'If we just split the text with pure python we have {len(lyrics.split())} words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db33ffd8-1005-4f06-ab61-17d3ebfbe731",
   "metadata": {},
   "source": [
    "**Why is there a difference in the word's list?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e005ec7-27ae-4e8b-a711-e28d0ddffa16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red,',\n",
       " 'spread;',\n",
       " 'night.',\n",
       " 'fall,',\n",
       " 'away,',\n",
       " 'beneath.',\n",
       " 'East;',\n",
       " 'gold.',\n",
       " 'day,',\n",
       " 'heath,',\n",
       " 'leaf:',\n",
       " 'light.',\n",
       " 'ceased,',\n",
       " 'height,',\n",
       " 'tall.',\n",
       " 'released.',\n",
       " 'hall!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(lyrics.split()) - set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fe768be-0e48-4aea-938b-c32d5dd636f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['East',\n",
       " 'spread',\n",
       " ',',\n",
       " 'hall',\n",
       " 'leaf',\n",
       " 'height',\n",
       " 'fall',\n",
       " '.',\n",
       " '!',\n",
       " 'red',\n",
       " 'released',\n",
       " ';',\n",
       " 'ceased',\n",
       " 'gold',\n",
       " 'beneath',\n",
       " 'heath',\n",
       " ':',\n",
       " 'tall',\n",
       " 'light']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(words) - set(lyrics.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb9ae7c-cd89-4ca9-8f8e-21c398a31760",
   "metadata": {},
   "source": [
    "We cam see the tokenizer removed chars like `!` and ignored 'words' like `:`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17054f4-d83e-4f4c-8c11-cb8c1f92d482",
   "metadata": {},
   "source": [
    "# Lemmatizing and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b712c3-a123-42be-b059-85fb70eb35b0",
   "metadata": {},
   "source": [
    "#### **What is the difference between lemmatizing and stemming?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea86dc4-0df0-40a1-88bf-8d528ce52034",
   "metadata": {},
   "source": [
    "**Stemming uses the stem of the word, while lemmatization uses the context in which the word is being used.**\n",
    "  \n",
    "Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word.\n",
    "\n",
    "Lemmatization takes into consideration the morphological analysis of the words. It is necessary to have dictionaries which the algorithm can look through to link the form back to its lemma. \n",
    "* Stemming has its application in Sentiment Analysis while Lemmatization has its application in Chatbots, human-answering.  \n",
    "* For instance, stemming the word ‘Caring‘ would return ‘Car‘, Lemmatizing the word ‘Caring‘ would return ‘Care‘."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e1a23-793b-41f7-ad5b-36ae9dc6b559",
   "metadata": {},
   "source": [
    "Lemmatization keeps the meaning of the word,  \n",
    "Stemming transform the word by rules.  \n",
    "  \n",
    "If we need the meaning - use Lemmatization, if not - Stemming is much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb55a3b-0084-437a-8a09-61eae8dc1683",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "488c4be9-a6c7-42e9-bca6-f655fd6fa2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 92 unique words after the stemming\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "print(f'There are {len(set(stemmed))} unique words after the stemming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e8b578d-04b9-4628-87bf-cb72a81dc602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed words:\n",
      " ['far', 'over', 'the', 'misti', 'mountain', 'cold', 'to', 'dungeon', 'deep', 'and', 'cavern', 'old'] ...\n"
     ]
    }
   ],
   "source": [
    "print('The stemmed words:\\n' ,stemmed[:12] , '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a5ff8-0a52-45ba-8343-2a57dfe7110a",
   "metadata": {},
   "source": [
    "We can compare some of the stemmed words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40e470f6-58cd-4ae2-936e-259f216159fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Far': 'far',\n",
       " 'over': 'over',\n",
       " 'the': 'the',\n",
       " 'Misty': 'misti',\n",
       " 'Mountains': 'mountain',\n",
       " 'cold': 'cold',\n",
       " 'To': 'to',\n",
       " 'dungeons': 'dungeon'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_and_after = {words[idx]: stemmed[idx] for idx in range(8)} \n",
    "before_and_after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92948a7-a665-4276-9a12-4e262ca00b69",
   "metadata": {},
   "source": [
    "There are a few changes that have been made to the words.  \n",
    "for example, we can see that the ending of the word got changed.  \n",
    "some 'y' 'i' 'e' 'ing' were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960224a7-5fe6-45fb-bf3c-3aaa3969764f",
   "metadata": {},
   "source": [
    "## Stemmer implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4a53a-45ac-4e19-b6db-d7878c1d929d",
   "metadata": {},
   "source": [
    "Here is an implementation of a simple stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a641285-38e4-45eb-986f-5d2e4f3453a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(string):\n",
    "# cutting the end of a verb\n",
    "    if string[-3:] in ['ize', 'ing']:\n",
    "        return string[:-3]\n",
    "\n",
    "# cutting the end of a noun\n",
    "    if string[-4:] in ['ment', 'ship'] and string not in ['ment', 'ship']: \n",
    "        return string[:-4]\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20cc83f1-a4f7-40a6-be50-7f9cb9cff254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whistl'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_stemmed = [stemmer(word) for word in words]\n",
    "before_and_after_imp = {words[idx]: imp_stemmed[idx] for idx in range(len(words))} \n",
    "before_and_after_imp['whistling']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a955288-20a7-4000-8dab-6227080375f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48627719-1f5f-4412-8c2b-f22b3bebbec7",
   "metadata": {},
   "source": [
    "We will use the library Spacy to lemmatize the text and compare the output to the stemming performed above. First we load the default Spacy model for English.  \n",
    "This contains Spacy's saved data about how to process English text. Now we will use this to lemmatize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b95c2336-5723-4d76-8f79-c636be62e29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 94 unique words after the lemmatization\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "lem = [string.lemma_.lower() for string in nlp(lyrics)]\n",
    "print(f'There are {len(set(lem))} unique words after the lemmatization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c4b30a-e371-40c1-ac25-c1c5c0205db3",
   "metadata": {},
   "source": [
    "And now lets see some before and after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3895938-1b8d-4acb-ad0a-483ba16d0dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Far': 'far',\n",
       " 'over': 'over',\n",
       " 'the': 'the',\n",
       " 'Misty': 'misty',\n",
       " 'Mountains': 'mountains',\n",
       " 'cold': 'cold',\n",
       " 'To': 'to',\n",
       " 'dungeons': 'dungeon',\n",
       " 'deep': 'deep',\n",
       " 'and': 'and',\n",
       " 'caverns': 'cavern',\n",
       " 'old': 'old',\n",
       " 'We': 'we'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_and_after_lem = {words[idx]: lem[idx] for idx in range(13)}\n",
    "before_and_after_lem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc54c7-1b51-49bc-8b82-c4981da6c396",
   "metadata": {},
   "source": [
    "Not as what happened when stemming,  \n",
    "`Misty` became `misty` and with stemming it transformed to `misti`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd356e75-295d-4e2a-b0b4-2445070b3e14",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4a00a-0103-437a-a2c7-db3fd558bf51",
   "metadata": {},
   "source": [
    "Removing the stop words (which don't contribute to the text's meaning) can reduce the noise for the nlp models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "880e1833-2f4a-4198-afca-54f318f390d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jrock/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1006b50-a180-4b17-b6b2-800f3b91f757",
   "metadata": {},
   "source": [
    "Let's have a look on the list of the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53c64fdb-8d86-431e-bb0b-7988361b295a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9924a3-2ed3-4d20-991f-835e38260ec5",
   "metadata": {},
   "source": [
    "And filter the lyrics from stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39938044-f218-40ed-8ed0-fff6012864c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the first 10 words in the lyrics:\n",
      "['Far', 'over', 'the', 'Misty', 'Mountains', 'cold', 'To', 'dungeons', 'deep', 'and']\n",
      "Here are the first 10 words in the filterd lyrics:\n",
      "['Far', 'Misty', 'Mountains', 'cold', 'dungeons', 'deep', 'caverns', 'old', 'must', 'away']\n"
     ]
    }
   ],
   "source": [
    "filtered_lyrics = [word for word in words if not word.lower() in stop_words]\n",
    "print(f'Here are the first 10 words in the lyrics:\\n{words[:10]}')\n",
    "print(f'Here are the first 10 words in the filterd lyrics:\\n{filtered_lyrics[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b769829-5cc0-4c1d-834a-252b4a1ab17d",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77edd48-94d1-4645-9d7c-d122e3874e02",
   "metadata": {},
   "source": [
    "We will now see how a sentence can be transformed into a feature vector using a bag of words model.  \n",
    "We can represent each word as a one-hot encoded vector (with a single 1 in the column for that word), and add their vectors together to get the feature vector for a sentence:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b53784a-79e8-45ca-aaee-96aa211793af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 90)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(lyrics.split('.'))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373304a-bdc7-4976-a654-fb4716e70c12",
   "metadata": {},
   "source": [
    "What do the rows and columns of the feature matrix X represent?  \n",
    "\n",
    "A column is a word, a row is a sentence.  \n",
    "The values are the number of times the word appear per sentence.   \n",
    "The second column is the word 'document' and the third is 'first'.  \n",
    "note: the model doesnt save the order of the wards, therefor, bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47dceb99-026e-4a19-a25c-9228df5c7fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 sentences in the text and 90 unique words\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {X.shape[0]} sentences in the text and {X.shape[1]} unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50764234-e929-4099-bb89-2875a9ec7e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at index 0, the word is across\n",
      "at index 1, the word is all\n",
      "at index 2, the word is and\n",
      "at index 3, the word is away\n",
      "at index 4, the word is be\n",
      "at index 5, the word is beneath\n",
      "at index 6, the word is blazed\n",
      "at index 7, the word is blow\n",
      "at index 8, the word is break\n",
      "at index 9, the word is but\n",
      "at index 10, the word is call\n",
      "at index 11, the word is caverns\n",
      "at index 12, the word is ceased\n",
      "at index 13, the word is cold\n",
      "at index 14, the word is crept\n",
      "at index 15, the word is dark\n",
      "at index 16, the word is day\n",
      "at index 17, the word is deep\n",
      "at index 18, the word is dungeons\n",
      "at index 19, the word is east\n",
      "at index 20, the word is ere\n",
      "at index 21, the word is fall\n",
      "at index 22, the word is far\n",
      "at index 23, the word is farewell\n",
      "at index 24, the word is find\n",
      "at index 25, the word is fire\n",
      "at index 26, the word is flaming\n",
      "at index 27, the word is forest\n",
      "at index 28, the word is forgotten\n",
      "at index 29, the word is from\n",
      "at index 30, the word is gold\n",
      "at index 31, the word is hall\n",
      "at index 32, the word is harsh\n",
      "at index 33, the word is hearth\n",
      "at index 34, the word is heath\n",
      "at index 35, the word is height\n",
      "at index 36, the word is in\n",
      "at index 37, the word is it\n",
      "at index 38, the word is its\n",
      "at index 39, the word is lay\n",
      "at index 40, the word is leaf\n",
      "at index 41, the word is light\n",
      "at index 42, the word is like\n",
      "at index 43, the word is long\n",
      "at index 44, the word is marsh\n",
      "at index 45, the word is may\n",
      "at index 46, the word is misty\n",
      "at index 47, the word is moaning\n",
      "at index 48, the word is mountain\n",
      "at index 49, the word is mountains\n",
      "at index 50, the word is movement\n",
      "at index 51, the word is must\n",
      "at index 52, the word is night\n",
      "at index 53, the word is no\n",
      "at index 54, the word is of\n",
      "at index 55, the word is old\n",
      "at index 56, the word is on\n",
      "at index 57, the word is or\n",
      "at index 58, the word is our\n",
      "at index 59, the word is over\n",
      "at index 60, the word is pines\n",
      "at index 61, the word is rain\n",
      "at index 62, the word is red\n",
      "at index 63, the word is released\n",
      "at index 64, the word is roaring\n",
      "at index 65, the word is shadows\n",
      "at index 66, the word is shrill\n",
      "at index 67, the word is silent\n",
      "at index 68, the word is spread\n",
      "at index 69, the word is stirred\n",
      "at index 70, the word is tall\n",
      "at index 71, the word is the\n",
      "at index 72, the word is there\n",
      "at index 73, the word is things\n",
      "at index 74, the word is though\n",
      "at index 75, the word is to\n",
      "at index 76, the word is torches\n",
      "at index 77, the word is trees\n",
      "at index 78, the word is voices\n",
      "at index 79, the word is was\n",
      "at index 80, the word is we\n",
      "at index 81, the word is went\n",
      "at index 82, the word is were\n",
      "at index 83, the word is west\n",
      "at index 84, the word is whistling\n",
      "at index 85, the word is wind\n",
      "at index 86, the word is winds\n",
      "at index 87, the word is with\n",
      "at index 88, the word is withered\n",
      "at index 89, the word is wood\n"
     ]
    }
   ],
   "source": [
    "for idx, word in enumerate(vectorizer.get_feature_names_out()):\n",
    "    print(f'at index {idx}, the word is {word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042bbf96-ebc7-4ad0-9785-93546ba18e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

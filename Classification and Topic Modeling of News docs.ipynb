{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Topic Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxXPmYWLroVT",
    "tags": []
   },
   "source": [
    "## Classification - Hot News\n",
    "\n",
    "In this example I will perform document classification in order to predict the category of news articles from the Reuters Corpus using a **bag-of-words** model and **one-hot encoding**. Lastly, I will perform topic modeling with **LDA** to see whether we can predict the categories of news articles without any labelled data.\n",
    "\n",
    "\n",
    "## The Reuters Corpus\n",
    "\n",
    "The Reuters Corpus is a collection of news documents along with category tags that are commonly used to test document classification. It is split into two sets: the *training* documents used to train a classification algorithm, and the *test* documents used to test the classifier's performance.\n",
    "\n",
    "The Reuters Corpus is accessible through NLTK; for more information see the [NLTK Corpus HOWTO](http://www.nltk.org/howto/corpus.html#categorized-corpora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/jrock/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jrock/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many documents are in the Reuters Corpus?\n",
      "There are 10788 articles,7769 for training and 3019 for testing.\n"
     ]
    }
   ],
   "source": [
    "ids_train = list(filter(lambda article: article.startswith(\"train\"), reuters.fileids()))\n",
    "ids_test = list(filter(lambda article: article.startswith(\"test\"), reuters.fileids()))\n",
    "print('How many documents are in the Reuters Corpus?')\n",
    "print(f'There are {len(reuters.fileids())} articles,{len(ids_train)} for training and {len(ids_test)} for testing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1253696 words in the train data set, and 467205 in the test data set.\n"
     ]
    }
   ],
   "source": [
    "words_train = sum(nltk.FreqDist(reuters.words(ids_train)).values())\n",
    "words_test = sum(nltk.FreqDist(reuters.words(ids_test)).values())\n",
    "print(f'There are {words_train} words in the train data set, and {words_test} in the test data set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The five most common categories in the training documents are:\n",
      "['earn', 'acq', 'money-fx', 'grain', 'crude']\n"
     ]
    }
   ],
   "source": [
    "cats = reuters.categories(ids_train)\n",
    "counts = {}\n",
    "for c in cats:\n",
    "    counts[c] = len(list(filter(lambda article: article.startswith('train'), reuters.fileids(categories=c))))\n",
    "print(f'The five most common categories in the training documents are:\\n{sorted(counts, key=counts.get, reverse=True)[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WA8pQjnp7I96"
   },
   "source": [
    "## Classifying Reuters (supervised) using LinearSVC (SVM)\n",
    "\n",
    "Now I will put these together in order to build a classifier for Reuters articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "text_train = [reuters.raw(i) for i in ids_train]\n",
    "text_test = [reuters.raw(i) for i in ids_test]\n",
    "\n",
    "# Converting the training and testing documents into matrices X and X2 of feature vectors using CountVectorizer()\n",
    "X = vectorizer.fit_transform(text_train)\n",
    "X2 = vectorizer.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "cat_train = []\n",
    "for id in ids_train:\n",
    "    cat_train.append(reuters.categories(fileids=[id]))\n",
    "    \n",
    "cat_test = []\n",
    "for id in ids_test:\n",
    "    cat_test.append(reuters.categories(fileids=[id]))\n",
    "# Convert the category labels into matrices y and y2 of binary features for classification using MultiLabelBinarizer() from scikit-learn\n",
    "labeler = MultiLabelBinarizer()\n",
    "y = labeler.fit_transform(cat_train)\n",
    "y2 = labeler.transform(cat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to fit a multiclass SVM classifier on the training data\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = OneVsRestClassifier(LinearSVC())\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       719\n",
      "           1       1.00      0.39      0.56        23\n",
      "           2       1.00      0.64      0.78        14\n",
      "           3       0.78      0.70      0.74        30\n",
      "           4       0.92      0.67      0.77        18\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       1.00      0.83      0.91        18\n",
      "           7       0.00      0.00      0.00         2\n",
      "           8       0.00      0.00      0.00         3\n",
      "           9       0.93      0.93      0.93        28\n",
      "          10       1.00      0.78      0.88        18\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.91      0.86      0.88        56\n",
      "          13       1.00      0.50      0.67        20\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.70      0.50      0.58        28\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.84      0.86      0.85       189\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.78      0.82      0.80        44\n",
      "          20       0.00      0.00      0.00         4\n",
      "          21       0.97      0.98      0.98      1087\n",
      "          22       0.67      0.20      0.31        10\n",
      "          23       1.00      0.53      0.69        17\n",
      "          24       0.90      0.80      0.85        35\n",
      "          25       0.92      0.73      0.81        30\n",
      "          26       0.90      0.81      0.86       149\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       0.50      0.60      0.55         5\n",
      "          30       1.00      0.33      0.50         6\n",
      "          31       1.00      0.75      0.86         4\n",
      "          32       1.00      0.29      0.44         7\n",
      "          33       0.00      0.00      0.00         1\n",
      "          34       0.81      0.69      0.75       131\n",
      "          35       0.75      1.00      0.86        12\n",
      "          36       0.70      0.50      0.58        14\n",
      "          37       0.00      0.00      0.00         1\n",
      "          38       0.71      0.57      0.63        21\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       1.00      0.57      0.73        14\n",
      "          41       1.00      1.00      1.00         3\n",
      "          42       0.00      0.00      0.00         1\n",
      "          43       0.56      0.42      0.48        24\n",
      "          44       0.00      0.00      0.00         6\n",
      "          45       0.80      0.21      0.33        19\n",
      "          46       0.74      0.72      0.73       179\n",
      "          47       0.72      0.76      0.74        34\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.77      0.57      0.65        30\n",
      "          50       0.00      0.00      0.00         1\n",
      "          51       0.00      0.00      0.00         2\n",
      "          52       0.00      0.00      0.00         2\n",
      "          53       0.17      0.33      0.22         6\n",
      "          54       0.69      0.62      0.65        47\n",
      "          55       1.00      0.64      0.78        11\n",
      "          56       0.00      0.00      0.00         1\n",
      "          57       1.00      0.50      0.67        10\n",
      "          58       0.00      0.00      0.00         1\n",
      "          59       0.00      0.00      0.00        12\n",
      "          60       1.00      0.14      0.25         7\n",
      "          61       1.00      0.33      0.50         3\n",
      "          62       0.00      0.00      0.00         3\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         3\n",
      "          65       1.00      0.44      0.62         9\n",
      "          66       0.86      0.67      0.75        18\n",
      "          67       1.00      0.50      0.67         2\n",
      "          68       1.00      0.46      0.63        24\n",
      "          69       1.00      0.67      0.80        12\n",
      "          70       0.00      0.00      0.00         1\n",
      "          71       0.77      0.69      0.73        89\n",
      "          72       1.00      0.50      0.67         8\n",
      "          73       0.67      0.20      0.31        10\n",
      "          74       1.00      0.15      0.27        13\n",
      "          75       0.33      0.09      0.14        11\n",
      "          76       0.80      0.61      0.69        33\n",
      "          77       0.00      0.00      0.00        11\n",
      "          78       0.93      0.78      0.85        36\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.00      0.00      0.00         2\n",
      "          81       0.00      0.00      0.00         5\n",
      "          82       0.00      0.00      0.00         4\n",
      "          83       1.00      0.58      0.74        12\n",
      "          84       0.70      0.71      0.71       117\n",
      "          85       0.86      0.51      0.64        37\n",
      "          86       0.89      0.82      0.85        71\n",
      "          87       0.90      0.90      0.90        10\n",
      "          88       0.38      0.21      0.27        14\n",
      "          89       1.00      0.38      0.56        13\n",
      "\n",
      "   micro avg       0.90      0.81      0.85      3744\n",
      "   macro avg       0.56      0.39      0.44      3744\n",
      "weighted avg       0.88      0.81      0.83      3744\n",
      " samples avg       0.87      0.86      0.86      3744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y2, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1APVAlin4drp"
   },
   "source": [
    "## Topic Modeling with LatentDirichletAllocation (LDA)\n",
    "\n",
    "Now we will see if we can use topic modeling to discover the topics in the Reuters news articles without using the labels provided in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jrock/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))  #Exclude stopwords\n",
    "X = vectorizer.fit_transform(text_train + text_test) # Encode the articles as a matrix of feature vectors using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10788x30778 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 629849 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column should be a word, and each row should be a document. The cells should contain the number of times the word appears in the given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation # Creating a model *lda* by using scikit-learn's LatentDirichletAllocation to model the topics in the documents. \n",
    "lda = LatentDirichletAllocation(n_components=len(cats)) # Setting the argument *n_components* to equal the number of categories in Reuters \n",
    "res = lda.fit_transform(X)  # Using the matrix X as input to the model's *fit_transform()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10788, 90)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the output of this function represent?  \n",
    "we get a probability for each document to be each of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.30687831e-05, 4.85713811e-02, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.30687831e-05, 3.40484451e-02,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 2.23323268e-02, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.30687831e-05, 3.30687831e-05,\n",
       "       1.15231963e-01, 3.30687831e-05, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.30687831e-05, 2.30192269e-02,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 2.73691399e-01, 3.42676719e-02,\n",
       "       3.30687831e-05, 3.30687831e-05, 7.27632609e-02, 1.84704051e-01,\n",
       "       3.30687831e-05, 5.30953241e-02, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.21354906e-02, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 5.10921034e-02, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 3.30687831e-05, 3.30687831e-05, 3.30687831e-05,\n",
       "       3.30687831e-05, 5.25010602e-02])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0] # Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res[0]) # The number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most prominent topic for document 0 is cat number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Classification - Hot News.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

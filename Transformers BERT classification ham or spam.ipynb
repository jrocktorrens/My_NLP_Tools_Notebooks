{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lv4h7W-AvtLW"
   },
   "source": [
    "# Exercise: Fine-tuning transformers\n",
    "\n",
    "In this exercise we will fine-tune the BERT transformer model to achieve impressive results on various NLP tasks.\n",
    "\n",
    "**Note:** It is recommended to run this notebook in Google Colab using GPU acceleration (Runtime > Change runtime type > GPU).\n",
    "\n",
    "We will use the `transformers` library from [HuggingFace](https://huggingface.co/), which provides a flexible, high-level API for applying state-of-the-art transformer models with minimal boilerplate code. Run the following line to download `transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMXnR0QMapoK",
    "outputId": "247a9efd-2934-4f61-ec7a-2e4780f2aaa1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use at the beginning of a notebook in order to use GPU\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcGI3KZd-QL0",
    "outputId": "a25ebede-531e-4c84-9e99-a9b6b90b5575"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets==4.4.0 in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==4.4.0) (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==4.4.0) (3.17.3)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==4.4.0) (1.2.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==4.4.0) (1.1.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==4.4.0) (21.2.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==4.4.0) (1.15.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==4.4.0) (5.2.2)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==4.4.0) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==4.4.0) (1.19.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==4.4.0) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==4.4.0) (4.62.3)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==4.4.0) (2.3)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==4.4.0) (0.3.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==4.4.0) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets==4.4.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets==4.4.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets==4.4.0) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets==4.4.0) (1.24.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow_datasets==4.4.0) (3.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets==4.4.0) (1.53.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets==4.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2n8YgeXTUed",
    "outputId": "c693b5d3-c12a-421a-83e7-6eb2f81081ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIdzxGtMMEtW"
   },
   "source": [
    "# Part 1: Microsoft Research Paraphrase Corpus\n",
    "\n",
    "We will begin by fine-tuning BERT to classify sentence pairs from the Microsoft Research Paraphrase Corpus (MRPC) dataset, one of the standard [GLUE Benchmark](https://gluebenchmark.com/) tasks used to evaluate NLP models.\n",
    "\n",
    "We can load this dataset conveniently using the TensorFlow Datasets API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ei9_Ks1uYE_E"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "mrpc_data, mrpc_info = tfds.load('glue/mrpc', with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNN7CdF-YOlc"
   },
   "source": [
    "## Questions:\n",
    "1. Examine `mrpc_info`. What is the size of the MRPC dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HR0mOlsTapoS",
    "outputId": "d356a60a-5f97-47dd-c917-1a8cbd6d3269"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='glue',\n",
       "    full_name='glue/mrpc/2.0.0',\n",
       "    description=\"\"\"\n",
       "    GLUE, the General Language Understanding Evaluation benchmark\n",
       "    (https://gluebenchmark.com/) is a collection of resources for training,\n",
       "    evaluating, and analyzing natural language understanding systems.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    The Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) is a corpus of\n",
       "    sentence pairs automatically extracted from online news sources, with human annotations\n",
       "    for whether the sentences in the pair are semantically equivalent.\n",
       "    \"\"\",\n",
       "    homepage='https://www.microsoft.com/en-us/download/details.aspx?id=52398',\n",
       "    data_path='/root/tensorflow_datasets/glue/mrpc/2.0.0',\n",
       "    download_size=1.43 MiB,\n",
       "    dataset_size=1.74 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'idx': tf.int32,\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'sentence1': Text(shape=(), dtype=tf.string),\n",
       "        'sentence2': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    supervised_keys=None,\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=1725, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=3668, num_shards=1>,\n",
       "        'validation': <SplitInfo num_examples=408, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@inproceedings{dolan2005automatically,\n",
       "      title={Automatically constructing a corpus of sentential paraphrases},\n",
       "      author={Dolan, William B and Brockett, Chris},\n",
       "      booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},\n",
       "      year={2005}\n",
       "    }\n",
       "    @inproceedings{wang2019glue,\n",
       "      title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n",
       "      author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n",
       "      note={In the Proceedings of ICLR.},\n",
       "      year={2019}\n",
       "    }\n",
       "    \n",
       "    Note that each GLUE dataset has its own citation. Please see the source to see\n",
       "    the correct citation for each contained dataset.\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrpc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbG6g7IyapoU",
    "outputId": "9a4f6c94-3c6a-48dc-c91e-5afe503f95cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the MRPC dataset is 1.74 MiB\n"
     ]
    }
   ],
   "source": [
    "print(f\"The size of the MRPC dataset is {mrpc_info.dataset_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hu3L_jyuapoW"
   },
   "source": [
    "2. `mrpc_data['train']` and `mrpc_data['test']` are TensorFlow Dataset objects containing the train and test sets for MRPC. Use `mrpc_data['train'].take(6)` to view six samples from the train set. Hint: You can convert the object to a list with `list(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpKRl61BapoX",
    "outputId": "f45f3bb8-bd30-4f75-d2f4-04bf601a79f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'idx': <tf.Tensor: shape=(), dtype=int32, numpy=1680>,\n",
       "  'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       "  'sentence1': <tf.Tensor: shape=(), dtype=string, numpy=b'The identical rovers will act as robotic geologists , searching for evidence of past water .'>,\n",
       "  'sentence2': <tf.Tensor: shape=(), dtype=string, numpy=b'The rovers act as robotic geologists , moving on six wheels .'>},\n",
       " {'idx': <tf.Tensor: shape=(), dtype=int32, numpy=1456>,\n",
       "  'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       "  'sentence1': <tf.Tensor: shape=(), dtype=string, numpy=b\"Less than 20 percent of Boise 's sales would come from making lumber and paper after the OfficeMax purchase is completed .\">,\n",
       "  'sentence2': <tf.Tensor: shape=(), dtype=string, numpy=b\"Less than 20 percent of Boise 's sales would come from making lumber and paper after the OfficeMax purchase is complete , assuming those businesses aren 't sold .\">},\n",
       " {'idx': <tf.Tensor: shape=(), dtype=int32, numpy=3017>,\n",
       "  'label': <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       "  'sentence1': <tf.Tensor: shape=(), dtype=string, numpy=b'Spider-Man snatched $ 114.7 million in its debut last year and went on to capture $ 403.7 million .'>,\n",
       "  'sentence2': <tf.Tensor: shape=(), dtype=string, numpy=b'Spider-Man , rated PG-13 , snatched $ 114.7 million in its first weekend and went on to take in $ 403.7 million .'>},\n",
       " {'idx': <tf.Tensor: shape=(), dtype=int32, numpy=2896>,\n",
       "  'label': <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       "  'sentence1': <tf.Tensor: shape=(), dtype=string, numpy=b\"The 2002 second quarter results don 't include figures from our friends at Compaq .\">,\n",
       "  'sentence2': <tf.Tensor: shape=(), dtype=string, numpy=b'The year-ago numbers do not include figures from Compaq Computer .'>},\n",
       " {'idx': <tf.Tensor: shape=(), dtype=int32, numpy=499>,\n",
       "  'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       "  'sentence1': <tf.Tensor: shape=(), dtype=string, numpy=b'Solomon 5.5 is available initially in the United States and Canada , for a starting price of about $ 12,700 .'>,\n",
       "  'sentence2': <tf.Tensor: shape=(), dtype=string, numpy=b'Solomon 5.5 is now available in the U.S. and Canada through Microsoft Business Solutions resellers .'>},\n",
       " {'idx': <tf.Tensor: shape=(), dtype=int32, numpy=837>,\n",
       "  'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       "  'sentence1': <tf.Tensor: shape=(), dtype=string, numpy=b\"The family stopped for lunch at Freshwater Spit , where several children went to the water 's edge to play in the surf shortly after noon .\">,\n",
       "  'sentence2': <tf.Tensor: shape=(), dtype=string, numpy=b\"Several children , including the 8-year-old , went down to the water 's edge to play in the surf .\">}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mrpc_data['train'].take(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkjMriJSapoY"
   },
   "source": [
    "3. What are the input features and target variable in MRPC? What does the target variable represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-1rUWP2apoZ"
   },
   "source": [
    "The input features are two sentence pairs, sentence1 and sentence2.<br>\n",
    "The target variable ('label') represent whether the sentences in the pair are semantically equivalent or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cO2AixCad5d"
   },
   "source": [
    "We now will *tokenize* the input texts using WordPiece tokenization, so they can be used as input for BERT. We use the `BertTokenizer` provided by the `transformers` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zj4nxnczat6d"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0XGZgRibAD0"
   },
   "source": [
    "Let's examine the WordPiece tokenization of an arbitrary sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goh83uhta5BE",
    "outputId": "20e6dfd9-a04b-4324-a257-b7c44981c723"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 19081, 2024, 2004, 24826, 15683, 2135, 6179, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer(\"Transformers are astoundingly useful.\")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvZYevm2bRY1"
   },
   "source": [
    "## Questions:\n",
    "4. What do `input_ids` represent? Use `tokenizer.convert_ids_to_tokens(...)` to confirm your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gg_zK3c7apod"
   },
   "source": [
    "It seems like input_ids represent the indices words in the sentence. Lets try to confirm that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wC8fMfFDapoe",
    "outputId": "a893417a-e0b7-4f3e-fb30-5cd1818935c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'transformers',\n",
       " 'are',\n",
       " 'as',\n",
       " '##tou',\n",
       " '##nding',\n",
       " '##ly',\n",
       " 'useful',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(x['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p28GN9qxapof"
   },
   "source": [
    "Indeed, we can see that input_ids represent the indices of words in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1QBAGyfapof"
   },
   "source": [
    "5. What word from the input string was split into four \"subword tokens\" by the tokenizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yEYRHdqapog"
   },
   "source": [
    "The word 'astoundingly' was split into four \"subword tokens\":<br>\n",
    "'as',<br>\n",
    "'##tou',<br>\n",
    "'##nding',<br>\n",
    "'##ly',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxknJgI7dCYD"
   },
   "source": [
    "For GLUE tasks like MRPC, `transformers` contains a useful function `glue_convert_examples_to_features` that uses a tokenizer to convert samples into numeric features that can be used to train deep learning models. Run the following to create this numeric dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rH03O8DpdVX6",
    "outputId": "1c4eb3e2-9f3f-4909-a69b-2dd75919acc8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/processors/glue.py:175: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import glue_convert_examples_to_features\n",
    "\n",
    "mrpc_data_train = glue_convert_examples_to_features(\n",
    "    mrpc_data['train'], tokenizer, max_length=128, task='mrpc')\n",
    "mrpc_data_test = glue_convert_examples_to_features(\n",
    "    mrpc_data['test'], tokenizer, max_length=128, task='mrpc')\n",
    "mrpc_data_train = mrpc_data_train.shuffle(100).batch(16).repeat(4)\n",
    "mrpc_data_test = mrpc_data_test.shuffle(100).batch(16).repeat(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ytZsYkHeMcf"
   },
   "source": [
    "## Questions:\n",
    "6. Use the same method as in question 2 to examine one element from `mrpc_data_train`. What are the dimensions of the input and output features for each batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6E07kPcAapoi",
    "outputId": "fe831e94-2e1f-465a-b52e-195d38fb14c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'attention_mask': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
       "   array([[1, 1, 1, ..., 0, 0, 0],\n",
       "          [1, 1, 1, ..., 0, 0, 0],\n",
       "          [1, 1, 1, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1, ..., 0, 0, 0],\n",
       "          [1, 1, 1, ..., 0, 0, 0],\n",
       "          [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n",
       "   'input_ids': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
       "   array([[  101,  4319,  9722, ...,     0,     0,     0],\n",
       "          [  101,  1037,  2976, ...,     0,     0,     0],\n",
       "          [  101,  2023,  2095, ...,     0,     0,     0],\n",
       "          ...,\n",
       "          [  101,  1999,  1037, ...,     0,     0,     0],\n",
       "          [  101,  2021,  2049, ...,     0,     0,     0],\n",
       "          [  101,  1047, 15909, ...,     0,     0,     0]], dtype=int32)>,\n",
       "   'token_type_ids': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
       "   array([[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>},\n",
       "  <tf.Tensor: shape=(16,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0])>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mrpc_data_train.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S87XC9sgapoj"
   },
   "source": [
    "The input dimension of the features for each batch is (16, 128) and the output dimenstion is (16,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev2UFHx6apoj"
   },
   "source": [
    "7. What did `max_length=128` do above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61l9B4inapoj"
   },
   "source": [
    "The max_length=128 sets the maximum length of a sentence, thus sentences that are longer then 128 words will be truncated to fit 128 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZWNwxzje7P2"
   },
   "source": [
    "We can now build our BERT-based model by using the `TFBertForSequenceClassification` model from `transformers` (TF stands for \"TensorFlow\"): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSwQMZGefOKq",
    "outputId": "8b7f4cef-ea12-4b6b-afbc-aac0652dee51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "mrpc_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-T7XG6n-fYAn"
   },
   "source": [
    "## Questions\n",
    "8. Compile `mrpc_model` with the following arguments:\n",
    "  * Adam optimizer with learning rate `3e-5`\n",
    "   * Sparse Categorical Crossentropy loss function with `from_logits=True` (from_logits=True)`\n",
    "  * `metrics='accuracy'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HWvSbqYaapol"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "mrpc_model.compile(\n",
    "    optimizer=Adam(learning_rate=5e-5),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics='accuracy',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdoTs7nrapol"
   },
   "source": [
    "9. Train `mrpc_model` on the MRPC training data, using `mrpc_data_test` as validation data. What is the best validation accuracy you managed to achieve? **Hint:** Recommended training settings are `epochs=3, steps_per_epoch=64, validation_steps=16`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJCyxnMuapom",
    "outputId": "c33d7389-f0a9-4912-a4b3-e6a1900f1fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "64/64 [==============================] - 83s 958ms/step - loss: 0.6118 - accuracy: 0.6738 - val_loss: 0.3908 - val_accuracy: 0.9102\n",
      "Epoch 2/3\n",
      "64/64 [==============================] - 59s 917ms/step - loss: 0.5407 - accuracy: 0.7354 - val_loss: 0.4546 - val_accuracy: 0.6992\n",
      "Epoch 3/3\n",
      "64/64 [==============================] - 58s 915ms/step - loss: 0.4775 - accuracy: 0.7715 - val_loss: 0.4972 - val_accuracy: 0.7578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e90e3f510>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrpc_model.fit(x=mrpc_data_train, epochs=3, steps_per_epoch=64, validation_steps=16, validation_data=mrpc_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldT2Md9ZcEDU"
   },
   "source": [
    "The best validation accuracy was achieved at epoch 1 and its 0.9102\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6Lvo11DMMHn"
   },
   "source": [
    "# Part 2: Movie Reviews\n",
    "\n",
    "We will now fine-tune BERT on a new task that is not in the GLUE benchmark. The [IMDB Sentiment Analysis Dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) contains 50k texts of movie reviews classified as positive or negative.\n",
    "\n",
    "Load the file `IMDB Dataset.csv` as a Pandas DataFrame and split into train and test sets. **Note:** If you are running this notebook in Google Colab you must first upload the file to the runtime by using the \"Files\" tab on the left-hand side panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "91DSSPwVii1A"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "imdb_df = pd.read_csv('IMDB Dataset.csv.gz')\n",
    "imdb_df_train, imdb_df_test = train_test_split(imdb_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7XZxbPbkKdF"
   },
   "source": [
    "## Questions:\n",
    "10. How many samples are in the train and test sets? Is the dataset balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LBMcYx76apoo",
    "outputId": "3f31ff60-3ee2-44f5-f8e2-9c834d237c12",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45000 samples in the train set.\n",
      "There are 5000 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {imdb_df_train.shape[0]} samples in the train set.\")\n",
    "print(f\"There are {imdb_df_test.shape[0]} samples in the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-8VdYClapoo",
    "outputId": "bf66bcf1-e063-414c-ae00-9abef2c672ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    0.500422\n",
       "positive    0.499578\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the balance of the dataset:\n",
    "imdb_df_train.sentiment.value_counts()/imdb_df_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oAfQRe_apoo"
   },
   "source": [
    "We can see that the dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxce7m9Eapop"
   },
   "source": [
    "11. Print out one movie review and its label from the train set. Do you agree with the label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHz3aG6Iapop",
    "outputId": "78ec134a-be35-42f7-813a-14cd9f0134f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I agree that this is ONE of the very best episodes of the entire series--my only detraction would be the somewhat jarring appearance of Mark Lenard as the Romulan Commander. My reasoning is this--if you were not around for the first run of this episode, then you know Mr. Lenard as Sarek, Spock's father. And for the 2nd generation Trekkie (or Trekker--your preference) it takes you out of the scene at first. Yet he's an excellent commander as well as opposite for our captain and this episode is strongly written and well-acted by all. There are excellent points made by both sides about the cost of war vs.the price of peace and certainly does remind one of some of the best of the WWII and later era movies. Those are not my favorite genre but I certainly would recommend a fan of such to view this episode through that filter. You'll see it holds up. I'll never understand why Sci-Fi gets so little respect--the best drama comes out of placing ordinary people in extraordinary circumstances.\n"
     ]
    }
   ],
   "source": [
    "print(imdb_df_train['review'].iloc[77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Py-zu76zapop",
    "outputId": "5b0832c0-a238-4f2a-a133-9949c023a2e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "print(imdb_df_train['sentiment'].iloc[77])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJMoV50Qapop"
   },
   "source": [
    "I agree with the label. the review is obviously positive - as the writer writes: \"I agree that this is ONE of the very best episodes of the entire series\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__gbPypXnBfY"
   },
   "source": [
    "We now must convert the dataset to numerical features so that we use it to train our transformer model. The code below will convert the data to a TensorFlow Dataset object, but some lines are missing.\n",
    "\n",
    "## Questions:\n",
    "12. Fill in the marked line in the code below to set `label` equal to `1` if the review in the current row of the DataFrame is positive, and `0` if it is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVetXzCmapoq"
   },
   "source": [
    "13. Fill in the marked line in the code below to set `tokenized` equal to the WordPiece tokenization of the movie review in `text`. Use parameters `max_length=128, padding='max_length', truncation=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xBx4wAjUuO0k"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def imdb_gen(df):\n",
    "    def g():\n",
    "        for row in df.itertuples():\n",
    "            text = row.review\n",
    "            label = (1 if row.sentiment=='positive' else 0) ## ANSWER TO QUESTION 12 HERE\n",
    "            tokenized = tokenizer(text, max_length=128, padding='max_length', truncation=True) ## ANSWER TO QUESTION 13 HERE\n",
    "            yield {k: np.array(tokenized[k]) for k in tokenized}, label\n",
    "    return g\n",
    "\n",
    "input_names = ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "data_types = ({k: tf.int32 for k in input_names}, tf.int64)\n",
    "data_shapes = ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([]))\n",
    "\n",
    "imdb_data_train = tf.data.Dataset.from_generator(\n",
    "    imdb_gen(imdb_df_train),\n",
    "    data_types, data_shapes\n",
    ").shuffle(100).batch(32).repeat(4)\n",
    "\n",
    "imdb_data_test = tf.data.Dataset.from_generator(\n",
    "    imdb_gen(imdb_df_test),\n",
    "    data_types, data_shapes\n",
    ").shuffle(100).batch(32).repeat(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpHvhVYUapoq"
   },
   "source": [
    "14. Create a BERT-based classification model as in Part 1 (using the same optimizer, loss, and metric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2FV4BK-apoq",
    "outputId": "8891dfba-8d37-4022-b885-c464ace4785f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "mrpc_model2 = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "u0gOP5PAapor"
   },
   "outputs": [],
   "source": [
    "mrpc_model2.compile(\n",
    "    optimizer=Adam(learning_rate=5e-5),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiULnpTCapor"
   },
   "source": [
    "15. Train your model on `imdb_data_train`, using `imdb_data_test` as validation data. What is the best validation accuracy you managed to achieve? **Hint:** Recommended training settings are `epochs=10, steps_per_epoch=64, validation_steps=16`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTOMR94Mapor",
    "outputId": "076dc629-1162-4a25-98f8-6ed01c34e117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 148s 2s/step - loss: 0.5362 - accuracy: 0.7207 - val_loss: 0.3659 - val_accuracy: 0.8379\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 126s 2s/step - loss: 0.3759 - accuracy: 0.8296 - val_loss: 0.3324 - val_accuracy: 0.8574\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 125s 2s/step - loss: 0.3697 - accuracy: 0.8359 - val_loss: 0.3414 - val_accuracy: 0.8594\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 126s 2s/step - loss: 0.3569 - accuracy: 0.8433 - val_loss: 0.3688 - val_accuracy: 0.8262\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 126s 2s/step - loss: 0.3447 - accuracy: 0.8608 - val_loss: 0.3310 - val_accuracy: 0.8574\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 126s 2s/step - loss: 0.3220 - accuracy: 0.8628 - val_loss: 0.3250 - val_accuracy: 0.8730\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 126s 2s/step - loss: 0.3290 - accuracy: 0.8633 - val_loss: 0.2790 - val_accuracy: 0.8848\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 125s 2s/step - loss: 0.3005 - accuracy: 0.8755 - val_loss: 0.3150 - val_accuracy: 0.8574\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 126s 2s/step - loss: 0.3167 - accuracy: 0.8657 - val_loss: 0.2746 - val_accuracy: 0.8770\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 126s 2s/step - loss: 0.3077 - accuracy: 0.8701 - val_loss: 0.3055 - val_accuracy: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1db22f1790>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrpc_model2.fit(x=imdb_data_train, epochs=10, steps_per_epoch=64, validation_steps=16, validation_data=imdb_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IZrSVcyxcsq"
   },
   "source": [
    "The best validation accuracy was achieved at epoch 7 and its 0.8848\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mZDxvR-W79s"
   },
   "source": [
    "# Part 3: Kaggle\n",
    "\n",
    "We are now ready to apply the skills we have learned to the \"real world\". For this problem you may pick any [text classification dataset from Kaggle](https://www.kaggle.com/datasets?search=text+classification).\n",
    "\n",
    "If you are unsure of what dataset to use, you may start with the [spam text classification dataset](https://www.kaggle.com/team-ai/spam-text-message-classification), given in the accompanying file `SPAM text message 20170820 - Data.csv`.\n",
    "\n",
    "**Note:** If you are using Google Colab for this exercise, you may need to use a small (upwards of 20k samples) dataset to avoid memory limitations.\n",
    "\n",
    "## Questions:\n",
    "16. Load your dataset into a Pandas DataFrame `kaggle_df`, and split into train and test sets with `train_test_split`. Print out the number of samples in the train and test sets, check if the dataset is balanced, and examine a few examples of samples to understand your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "7T7eQgkjapos",
    "outputId": "5d70b07f-bdf1-4a4a-c42b-3d02fbd105a3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df = pd.read_csv('SPAM text message 20170820 - Data.csv')\n",
    "spam_df_train, spam_df_test = train_test_split(spam_df, test_size=0.1, random_state=42)\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VrGJl8LdyYzP",
    "outputId": "6eac0950-4021-461d-d012-f791b0bfe309",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5014 samples in the train set.\n",
      "There are 558 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {spam_df_train.shape[0]} samples in the train set.\")\n",
    "print(f\"There are {spam_df_test.shape[0]} samples in the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BilSfNfOyYzP",
    "outputId": "41bf5c0c-88b6-440d-d6ef-768a8192f514"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865576\n",
       "spam    0.134424\n",
       "Name: Category, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the balance of the dataset:\n",
    "spam_df_train.Category.value_counts()/spam_df_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPHLelmpy_8G"
   },
   "source": [
    "We can see that the dataset is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duGzFTMvGThD"
   },
   "source": [
    "Lets check two messages, one is a ham and the other is a spam: classifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1WP-LsfGSDV",
    "outputId": "98520345-0e57-46dd-e86a-381218370f70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K sure am in my relatives home. Sms me de. Pls:-)\n"
     ]
    }
   ],
   "source": [
    "print(spam_df_train['Message'].iloc[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MT1b6sASGSDX",
    "outputId": "d7027208-8ecf-4050-d0b5-307d4837cdd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n"
     ]
    }
   ],
   "source": [
    "print(spam_df_train['Category'].iloc[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NCsaDUMhGyFk",
    "outputId": "0f4c0142-113f-4a03-d523-ba3c783fde87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMV BONUS SPECIAL 500 pounds of genuine HMV vouchers to be won. Just answer 4 easy questions. Play Now! Send HMV to 86688 More info:www.100percent-real.com\n"
     ]
    }
   ],
   "source": [
    "print(spam_df_train['Message'].iloc[94])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VI4xM0-GyFw",
    "outputId": "44384958-b92e-405c-f20e-16a6ee9e44e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "print(spam_df_train['Category'].iloc[94])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9dj88egapos"
   },
   "source": [
    "17. Convert your dataframes into TensorFlow datasets as we did above for the IMDB dataset (including shuffling, grouping into batches of size 32, etc. as above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "SvTdiKLcapos"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def spam_gen(df):\n",
    "    def g():\n",
    "        for row in df.itertuples():\n",
    "            text = row.Message\n",
    "            label = (1 if row.Category=='spam' else 0) \n",
    "            tokenized = tokenizer(text, max_length=128, padding='max_length', truncation=True) \n",
    "            yield {k: np.array(tokenized[k]) for k in tokenized}, label\n",
    "    return g\n",
    "\n",
    "input_names = ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "data_types = ({k: tf.int32 for k in input_names}, tf.int64)\n",
    "data_shapes = ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([]))\n",
    "\n",
    "spam_data_train = tf.data.Dataset.from_generator(\n",
    "    spam_gen(spam_df_train),\n",
    "    data_types, data_shapes\n",
    ").shuffle(100).batch(32).repeat(4)\n",
    "\n",
    "spam_data_test = tf.data.Dataset.from_generator(\n",
    "    spam_gen(spam_df_test),\n",
    "    data_types, data_shapes\n",
    ").shuffle(100).batch(32).repeat(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1OmEqiNapos"
   },
   "source": [
    "18. Train a BERT-based classification model on your data. What is the best result you can achieve? Note: If your data is imbalanced, keep in mind what baseline accuracy you would expect if they model was guessing randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nAq36xBMapos",
    "outputId": "070715c0-0978-41d6-da06-a66fb7cfaf59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "kaggle_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "142zMd2lw8-1"
   },
   "outputs": [],
   "source": [
    "kaggle_model.compile(\n",
    "    optimizer=Adam(learning_rate=5e-5),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emHdkTyJw-OI",
    "outputId": "328aa505-e50e-4ace-ca49-4d9884f0d770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 128s 2s/step - loss: 0.1707 - accuracy: 0.9331 - val_loss: 0.0735 - val_accuracy: 0.9844\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 107s 2s/step - loss: 0.0526 - accuracy: 0.9834 - val_loss: 0.0445 - val_accuracy: 0.9883\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 106s 2s/step - loss: 0.0382 - accuracy: 0.9902 - val_loss: 0.0606 - val_accuracy: 0.9863\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 107s 2s/step - loss: 0.0241 - accuracy: 0.9951 - val_loss: 0.0516 - val_accuracy: 0.9883\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 106s 2s/step - loss: 0.0194 - accuracy: 0.9956 - val_loss: 0.0396 - val_accuracy: 0.9922\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 107s 2s/step - loss: 0.0114 - accuracy: 0.9980 - val_loss: 0.0396 - val_accuracy: 0.9922\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 107s 2s/step - loss: 0.0100 - accuracy: 0.9971 - val_loss: 0.0291 - val_accuracy: 0.9941\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 106s 2s/step - loss: 0.0128 - accuracy: 0.9971 - val_loss: 0.0521 - val_accuracy: 0.9902\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 107s 2s/step - loss: 0.0216 - accuracy: 0.9927 - val_loss: 0.0694 - val_accuracy: 0.9883\n",
      "Epoch 10/10\n",
      "52/64 [=======================>......] - ETA: 18s - loss: 0.0153 - accuracy: 0.9964WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 640 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 640 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 88s 1s/step - loss: 0.0153 - accuracy: 0.9964 - val_loss: 0.0588 - val_accuracy: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1dad1135d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_model.fit(x=spam_data_train, epochs=10, steps_per_epoch=64, validation_steps=16, validation_data=spam_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Km9hX6r3OwB"
   },
   "source": [
    "I've achieved a validation accuracy of 0.9941. \n",
    "If a baseline model will guess randomly stating a message is not spam, it will get an accuracy of 0.865 since that the amount of the mejority class, but this model will not detect a single spam message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YTzBZhRapos"
   },
   "source": [
    "19. Use your trained model to classify new text that you input yourself. See the commented code below for an example of how this could be done on the spam text classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-4AzA5xr42j",
    "outputId": "a9518eaa-eb62-452d-a441-8e6df6c74ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9983913849573582 probability that input text is spam\n",
      "0.001608615 probability that input text is ham\n"
     ]
    }
   ],
   "source": [
    "# Here is an example of classifying a spam message:\n",
    "from scipy.special import softmax\n",
    "tokenized = tokenizer(\"Make money while you are a sleep! Join our course and learn how to be a millionaire spending 5 minutes a day!\")\n",
    "logits = kaggle_model.predict({k: np.array(tokenized[k])[None] for k in input_names})[0]\n",
    "scores = softmax(logits, axis=1)[:, 1]\n",
    "print(1-scores[0], 'probability that input text is spam')\n",
    "print(scores[0], 'probability that input text is ham')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transformers exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
